{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 01: CartPole Basic Control\n",
    "\n",
    "This tutorial demonstrates how to train a reinforcement learning agent on the CartPole balancing task using Myriad's platform.\n",
    "\n",
    "## Learning Objectives\n",
    "1. Understand the CartPole environment structure\n",
    "2. Configure training runs using Hydra configs\n",
    "3. Train a DQN agent using the platform\n",
    "4. Analyze training and evaluation results\n",
    "5. Visualize training progress\n",
    "\n",
    "**The CartPole task:** Balance a pole on a moving cart by applying left/right forces. Goal: Keep the pole upright for as long as possible (max 500 steps).\n",
    "\n",
    "**Estimated runtime:** ~30-60 seconds on M1 CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from myriad.configs.default import (\n",
    "    AgentConfig,\n",
    "    Config,\n",
    "    EnvConfig,\n",
    "    RunConfig,\n",
    "    WandbConfig,\n",
    ")\n",
    "from myriad.envs.cartpole.tasks.control import make_env\n",
    "from myriad.platform import train_and_evaluate\n",
    "\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 1: Understanding the Environment\n",
    "\n",
    "Let's explore the CartPole environment structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Environment Configuration\n",
    "\n",
    "The environment has two main configurations:\n",
    "- **Physics Config:** Physical parameters (gravity, masses, pole length, etc.)\n",
    "- **Task Config:** Task parameters (max steps, termination thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_env()\n",
    "\n",
    "print(f\"Physics: {env.config.physics}\")\n",
    "print(f\"Task: {env.config.task}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Action and Observation Spaces\n",
    "\n",
    "**Actions:** Discrete(2)\n",
    "- 0 = push left\n",
    "- 1 = push right\n",
    "\n",
    "**Observations:** [x, x_dot, theta, theta_dot]\n",
    "- x: Cart position (m)\n",
    "- x_dot: Cart velocity (m/s)\n",
    "- theta: Pole angle from vertical (rad, 0=upright)\n",
    "- theta_dot: Pole angular velocity (rad/s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space = env.get_action_space(env.config)\n",
    "obs_shape = env.get_obs_shape(env.config)\n",
    "\n",
    "print(f\"Action space: Discrete({action_space.n})\")\n",
    "print(f\"Observation shape: {obs_shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Initial State\n",
    "\n",
    "The environment returns structured observations as a `PhysicsState` NamedTuple with named fields. This makes it easy to access specific components like `obs.theta` or `obs.x`.\n",
    "\n",
    "The platform automatically converts these to arrays for efficient training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(SEED)\n",
    "obs, state = env.reset(key, env.params, env.config)\n",
    "\n",
    "print(f\"Observation: {obs}\")\n",
    "print(f\"As array: {obs.to_array()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Taking Random Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step in range(3):\n",
    "    key, action_key, step_key = jax.random.split(key, 3)\n",
    "    action = action_space.sample(action_key)\n",
    "    next_obs, next_state, reward, done, _ = env.step(\n",
    "        step_key, state, action, env.params, env.config\n",
    "    )\n",
    "    \n",
    "    print(f\"Step {step + 1}: action={action}, theta={next_obs.theta:.4f} rad, reward={reward}, done={done}\")\n",
    "    \n",
    "    state, obs = next_state, next_obs\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 2: Training with the Platform\n",
    "\n",
    "Now let's train a DQN agent. The platform handles all the complexity:\n",
    "- Parallel environment execution (JAX vmap)\n",
    "- Replay buffer management\n",
    "- Agent updates and target network sync\n",
    "- Automatic environment resets\n",
    "- Metrics logging and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Configure the Training Run\n",
    "\n",
    "Create a configuration using Pydantic models (same structure as Hydra YAML configs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config(\n",
    "    env=EnvConfig(name=\"cartpole-control\"),\n",
    "    agent=AgentConfig(\n",
    "        name=\"dqn\",\n",
    "        learning_rate=1e-3,\n",
    "        gamma=0.99,\n",
    "        epsilon_start=1.0,\n",
    "        epsilon_end=0.05,\n",
    "        epsilon_decay_steps=10000,\n",
    "        target_network_frequency=500,\n",
    "        batch_size=64,\n",
    "    ),\n",
    "    run=RunConfig(\n",
    "        seed=SEED,\n",
    "        num_envs=16,\n",
    "        total_timesteps=50000,\n",
    "        buffer_size=10000,\n",
    "        log_frequency=1000,\n",
    "        eval_frequency=5000,\n",
    "        eval_rollouts=10,\n",
    "        eval_max_steps=500,\n",
    "        scan_chunk_size=10,\n",
    "    ),\n",
    "    wandb=WandbConfig(enabled=False),\n",
    ")\n",
    "\n",
    "print(f\"Training {config.run.total_timesteps:,} steps across {config.run.num_envs} parallel environments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Run Training\n",
    "\n",
    "**This will take ~30-60 seconds:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = train_and_evaluate(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Training Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total timesteps: {results.training_metrics.global_steps[-1]:,}\")\n",
    "print(f\"Evaluation checkpoints: {len(results.eval_metrics.global_steps)}\")\n",
    "\n",
    "if results.eval_metrics.mean_return:\n",
    "    print(f\"Final mean return: {results.eval_metrics.mean_return[-1]:.2f}\")\n",
    "    print(f\"Best mean return: {max(results.eval_metrics.mean_return):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 3: Analyzing Results\n",
    "\n",
    "The platform returns complete `TrainingResults` with all metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Training Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if results.training_metrics.loss:\n",
    "    print(f\"Initial loss: {results.training_metrics.loss[0]:.4f}\")\n",
    "    print(f\"Final loss: {results.training_metrics.loss[-1]:.4f}\")\n",
    "\n",
    "if \"td_error\" in results.training_metrics.agent_metrics:\n",
    "    td_errors = results.training_metrics.agent_metrics[\"td_error\"]\n",
    "    print(f\"Initial TD error: {td_errors[0]:.4f}\")\n",
    "    print(f\"Final TD error: {td_errors[-1]:.4f}\")\n",
    "\n",
    "if \"q_value\" in results.training_metrics.agent_metrics:\n",
    "    q_values = results.training_metrics.agent_metrics[\"q_value\"]\n",
    "    print(f\"Initial Q-value: {q_values[0]:.4f}\")\n",
    "    print(f\"Final Q-value: {q_values[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if results.eval_metrics.mean_return:\n",
    "    for i, (step, mean_ret, std_ret, mean_len) in enumerate(\n",
    "        zip(\n",
    "            results.eval_metrics.global_steps,\n",
    "            results.eval_metrics.mean_return,\n",
    "            results.eval_metrics.std_return,\n",
    "            results.eval_metrics.mean_length,\n",
    "        )\n",
    "    ):\n",
    "        print(f\"Checkpoint {i + 1} @ {step:,} steps: {mean_ret:.1f} ± {std_ret:.1f} (length: {mean_len:.0f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Performance Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 4: Visualizing Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle(\"CartPole DQN Training Results\", fontsize=16, fontweight=\"bold\")\n",
    "\n",
    "# Training Loss\n",
    "if results.training_metrics.loss:\n",
    "    ax = axes[0, 0]\n",
    "    ax.plot(results.training_metrics.global_steps, results.training_metrics.loss, linewidth=2, color=\"#2E86AB\")\n",
    "    ax.set_xlabel(\"Environment Steps\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.set_title(\"Training Loss\", fontweight=\"bold\")\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "# TD Error\n",
    "if \"td_error\" in results.training_metrics.agent_metrics:\n",
    "    ax = axes[0, 1]\n",
    "    ax.plot(results.training_metrics.global_steps, results.training_metrics.agent_metrics[\"td_error\"], linewidth=2, color=\"#A23B72\")\n",
    "    ax.set_xlabel(\"Environment Steps\")\n",
    "    ax.set_ylabel(\"TD Error\")\n",
    "    ax.set_title(\"Temporal Difference Error\", fontweight=\"bold\")\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "# Evaluation Returns\n",
    "if results.eval_metrics.mean_return:\n",
    "    ax = axes[1, 0]\n",
    "    steps = results.eval_metrics.global_steps\n",
    "    mean_ret = results.eval_metrics.mean_return\n",
    "    std_ret = results.eval_metrics.std_return\n",
    "    \n",
    "    ax.plot(steps, mean_ret, linewidth=2, color=\"#F18F01\", label=\"Mean Return\")\n",
    "    ax.fill_between(\n",
    "        steps,\n",
    "        np.array(mean_ret) - np.array(std_ret),\n",
    "        np.array(mean_ret) + np.array(std_ret),\n",
    "        alpha=0.3,\n",
    "        color=\"#F18F01\",\n",
    "        label=\"±1 Std Dev\",\n",
    "    )\n",
    "    ax.axhline(y=500, color=\"green\", linestyle=\"--\", alpha=0.7, label=\"Max (500)\")\n",
    "    ax.set_xlabel(\"Environment Steps\")\n",
    "    ax.set_ylabel(\"Episode Return\")\n",
    "    ax.set_title(\"Evaluation Performance\", fontweight=\"bold\")\n",
    "    ax.legend()\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "# Episode Lengths\n",
    "if results.eval_metrics.mean_length:\n",
    "    ax = axes[1, 1]\n",
    "    ax.plot(results.eval_metrics.global_steps, results.eval_metrics.mean_length, linewidth=2, color=\"#6A994E\")\n",
    "    ax.axhline(y=500, color=\"green\", linestyle=\"--\", alpha=0.7, label=\"Max (500)\")\n",
    "    ax.set_xlabel(\"Environment Steps\")\n",
    "    ax.set_ylabel(\"Episode Length\")\n",
    "    ax.set_title(\"Episode Length\", fontweight=\"bold\")\n",
    "    ax.legend()\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Tutorial Complete!\n",
    "\n",
    "## What You Learned\n",
    "1. How to explore CartPole environment structure\n",
    "2. How to configure training runs using `Config` objects\n",
    "3. How to train agents using `train_and_evaluate()`\n",
    "4. How to access and analyze `TrainingResults`\n",
    "5. How to visualize training progress\n",
    "\n",
    "## Next Steps\n",
    "Try experimenting with:\n",
    "- Different hyperparameters (`learning_rate`, `epsilon_decay`, etc.)\n",
    "- More parallel environments (`num_envs`)\n",
    "- Longer training (`total_timesteps`)\n",
    "- W&B logging (`wandb.enabled=True`)\n",
    "- Other agents (PPO, SAC) when available"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
