{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 02: Evaluating Classical Controllers\n",
    "\n",
    "This tutorial demonstrates how to use Myriad's platform to evaluate classical control strategies on the CartPole task.\n",
    "\n",
    "## Learning Objectives\n",
    "1. Use the platform's `evaluate()` function with `EvalConfig`\n",
    "2. Compare different control strategies using the platform\n",
    "3. Work with `EvaluationResults` objects\n",
    "4. Analyze and visualize results\n",
    "\n",
    "**Key Platform Feature:** The `evaluate()` function with `EvalConfig` provides a clean API for evaluation-only runs—perfect for classical controllers, pre-trained models, and baselines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from myriad.configs.default import AgentConfig, EnvConfig, EvalConfig\n",
    "from myriad.platform import evaluate\n",
    "\n",
    "# Set seaborn style\n",
    "sns.set_theme(style=\"whitegrid\", palette=\"muted\")\n",
    "\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 1: Classical Controllers Overview\n",
    "\n",
    "The platform includes classical controllers for baselines and debugging:\n",
    "\n",
    "1. **Random**: Uniform random action selection\n",
    "2. **Bang-Bang**: Threshold-based switching (if theta > 0, push right; else push left)\n",
    "3. **PID**: Proportional-Integral-Derivative control (for continuous actions)\n",
    "\n",
    "These are registered in the agent registry, so you can use them with `AgentConfig(name=\"random\")` just like learned agents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 2: Evaluating with EvalConfig\n",
    "\n",
    "The `evaluate()` function accepts `EvalConfig`—a simplified configuration focused exclusively on evaluation.\n",
    "\n",
    "**Why EvalConfig?**\n",
    "- No training-specific parameters (`num_envs`, `steps_per_env`, `scan_chunk_size`, etc.)\n",
    "- Clear intent: evaluation-only\n",
    "- Simpler configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Evaluate Random Controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results:\n",
      "  Mean return: 21.2\n",
      "  Std return: 11.0\n",
      "  Return range: [8.0, 62.0]\n",
      "  Mean episode length: 21.2\n",
      "\n",
      "Summary:\n",
      "{'mean_return': 21.25, 'std_return': 11.014876365661621, 'min_return': 8.0, 'max_return': 62.0, 'mean_length': 21.25, 'num_episodes': 100}\n"
     ]
    }
   ],
   "source": [
    "# Create evaluation config\n",
    "random_config = EvalConfig(\n",
    "    env=EnvConfig(name=\"cartpole-control\"),\n",
    "    agent=AgentConfig(name=\"random\"),\n",
    "    seed=SEED,\n",
    "    eval_rollouts=100,      # Run 100 episodes\n",
    "    eval_max_steps=500,     # Max 500 steps per episode\n",
    ")\n",
    "\n",
    "# Evaluate (with return_episodes=True to get trajectories)\n",
    "random_results = evaluate(config=random_config, return_episodes=True)\n",
    "\n",
    "# EvaluationResults has pre-computed statistics\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"  Mean return: {random_results.mean_return:.1f}\")\n",
    "print(f\"  Std return: {random_results.std_return:.1f}\")\n",
    "print(f\"  Return range: [{random_results.min_return:.1f}, {random_results.max_return:.1f}]\")\n",
    "print(f\"  Mean episode length: {random_results.mean_length:.1f}\")\n",
    "\n",
    "# Can also use summary()\n",
    "print(f\"\\nSummary:\")\n",
    "print(random_results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Evaluate Bang-Bang Controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results summary:\n",
      "\tmean_return: 42.01\n",
      "\tstd_return: 9.24\n",
      "\tmin_return: 24.00\n",
      "\tmax_return: 66.00\n",
      "\tmean_length: 42.01\n",
      "\tnum_episodes: 100.00\n"
     ]
    }
   ],
   "source": [
    "bangbang_config = EvalConfig(\n",
    "    env=EnvConfig(name=\"cartpole-control\"),\n",
    "    agent=AgentConfig(name=\"bangbang\"),\n",
    "    seed=SEED,\n",
    "    eval_rollouts=100,\n",
    "    eval_max_steps=500,\n",
    ")\n",
    "\n",
    "bangbang_results = evaluate(config=bangbang_config, return_episodes=True)\n",
    "\n",
    "print(f\"\\nResults summary:\")\n",
    "for k, v in bangbang_results.summary().items():\n",
    "    print(f\"\\t{k}: {v:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Evaluate PID Controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results summary:\n",
      "\tmean_return: 8.75\n",
      "\tstd_return: 0.46\n",
      "\tmin_return: 8.00\n",
      "\tmax_return: 10.00\n",
      "\tmean_length: 8.75\n",
      "\tnum_episodes: 100.00\n"
     ]
    }
   ],
   "source": [
    "pid_config = EvalConfig(\n",
    "    env=EnvConfig(name=\"cartpole-control\"),\n",
    "    agent=AgentConfig(name=\"pid\", kp=1.0, ki=0.0, kd=1.0, control_low=-1, control_high=1),\n",
    "    seed=SEED,\n",
    "    eval_rollouts=100,\n",
    "    eval_max_steps=500,\n",
    ")\n",
    "\n",
    "pid_results = evaluate(config=pid_config, return_episodes=True)\n",
    "\n",
    "print(f\"\\nResults summary:\")\n",
    "for k, v in pid_results.summary().items():\n",
    "    print(f\"\\t{k}: {v:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 3: Comparing Results\n",
    "\n",
    "EvaluationResults provides both summary statistics and raw data for custom analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performance Comparison:\n",
      "Controller  Mean Return  Std Return  Mean Length  Max Return\n",
      "    Random    21.250000   11.014876        21.25        62.0\n",
      " Bang-Bang    42.009998    9.236336        42.01        66.0\n",
      "       PID     8.750000    0.455522         8.75        10.0\n"
     ]
    }
   ],
   "source": [
    "comparison_df = pd.DataFrame({\n",
    "    \"Controller\": [\"Random\", \"Bang-Bang\", \"PID\"],\n",
    "    \"Mean Return\": [random_results.mean_return, bangbang_results.mean_return, pid_results.mean_return],\n",
    "    \"Std Return\": [random_results.std_return, bangbang_results.std_return, pid_results.std_return],\n",
    "    \"Mean Length\": [random_results.mean_length, bangbang_results.mean_length, pid_results.mean_length],\n",
    "    \"Max Return\": [random_results.max_return, bangbang_results.max_return, pid_results.max_return],\n",
    "})\n",
    "\n",
    "print(\"\\nPerformance Comparison:\")\n",
    "print(comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Distribution Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "fig.suptitle(\"Classical Controller Performance on CartPole\", fontsize=16, fontweight=\"bold\")\n",
    "\n",
    "# Episode returns (using raw data from results)\n",
    "ax = axes[0]\n",
    "data = [random_results.episode_returns, bangbang_results.episode_returns]\n",
    "bp = ax.boxplot(data, labels=[\"Random\", \"Bang-Bang\"], patch_artist=True)\n",
    "for patch, color in zip(bp[\"boxes\"], [\"lightblue\", \"lightcoral\"]):\n",
    "    patch.set_facecolor(color)\n",
    "ax.axhline(y=500, color=\"green\", linestyle=\"--\", linewidth=2, alpha=0.6, label=\"Max (500)\")\n",
    "ax.set_ylabel(\"Episode Return\", fontsize=12)\n",
    "ax.set_title(\"Return Distribution\", fontweight=\"bold\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Episode lengths\n",
    "ax = axes[1]\n",
    "data = [random_results.episode_lengths, bangbang_results.episode_lengths]\n",
    "bp = ax.boxplot(data, labels=[\"Random\", \"Bang-Bang\"], patch_artist=True)\n",
    "for patch, color in zip(bp[\"boxes\"], [\"lightblue\", \"lightcoral\"]):\n",
    "    patch.set_facecolor(color)\n",
    "ax.axhline(y=500, color=\"green\", linestyle=\"--\", linewidth=2, alpha=0.6, label=\"Max (500)\")\n",
    "ax.set_ylabel(\"Episode Length\", fontsize=12)\n",
    "ax.set_title(\"Length Distribution\", fontweight=\"bold\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "fig.suptitle(\"Episode Return Distributions\", fontsize=16, fontweight=\"bold\")\n",
    "\n",
    "# Random\n",
    "ax = axes[0]\n",
    "ax.hist(random_results.episode_returns, bins=20, alpha=0.7, color=\"lightblue\", edgecolor=\"black\")\n",
    "ax.axvline(\n",
    "    random_results.mean_return, color=\"blue\", linestyle=\"--\", linewidth=2,\n",
    "    label=f\"Mean: {random_results.mean_return:.1f}\"\n",
    ")\n",
    "ax.set_xlabel(\"Episode Return\")\n",
    "ax.set_ylabel(\"Frequency\")\n",
    "ax.set_title(\"Random Controller\", fontweight=\"bold\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Bang-Bang\n",
    "ax = axes[1]\n",
    "ax.hist(bangbang_results.episode_returns, bins=20, alpha=0.7, color=\"lightcoral\", edgecolor=\"black\")\n",
    "ax.axvline(\n",
    "    bangbang_results.mean_return, color=\"red\", linestyle=\"--\", linewidth=2,\n",
    "    label=f\"Mean: {bangbang_results.mean_return:.1f}\"\n",
    ")\n",
    "ax.set_xlabel(\"Episode Return\")\n",
    "ax.set_ylabel(\"Frequency\")\n",
    "ax.set_title(\"Bang-Bang Controller\", fontweight=\"bold\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 4: Analyzing Trajectories\n",
    "\n",
    "Since we used `return_episodes=True`, the results include full trajectories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Trajectory data available in results.episodes:\")\n",
    "print(f\"  Keys: {list(bangbang_results.episodes.keys())}\")\n",
    "print(f\"\\nShapes:\")\n",
    "for key, arr in bangbang_results.episodes.items():\n",
    "    print(f\"  {key}: {arr.shape}\")\n",
    "\n",
    "print(f\"\\nFormat: (num_episodes={bangbang_results.num_episodes}, max_steps={bangbang_config.eval_max_steps}, ...)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Visualize a Single Episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick the first episode\n",
    "episode_idx = 0\n",
    "length = int(bangbang_results.episode_lengths[episode_idx])\n",
    "\n",
    "# Extract trajectory data (trim to actual episode length)\n",
    "obs = bangbang_results.episodes[\"observations\"][episode_idx, :length, :]\n",
    "actions = bangbang_results.episodes[\"actions\"][episode_idx, :length]\n",
    "\n",
    "# Parse observations: [x, x_dot, theta, theta_dot]\n",
    "x = obs[:, 0]\n",
    "theta = obs[:, 2]\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(3, 1, figsize=(12, 8), sharex=True)\n",
    "fig.suptitle(\n",
    "    f\"Bang-Bang Controller - Episode {episode_idx} (Length: {length}, Return: {bangbang_results.episode_returns[episode_idx]:.1f})\",\n",
    "    fontsize=14, fontweight=\"bold\"\n",
    ")\n",
    "\n",
    "# Pole angle\n",
    "ax = axes[0]\n",
    "ax.plot(theta, linewidth=2, color=\"crimson\")\n",
    "ax.axhline(y=0, color=\"gray\", linestyle=\"--\", alpha=0.5)\n",
    "ax.set_ylabel(\"Pole Angle (rad)\")\n",
    "ax.set_title(\"Pole Angle Over Time\", fontweight=\"bold\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Cart position\n",
    "ax = axes[1]\n",
    "ax.plot(x, linewidth=2, color=\"crimson\")\n",
    "ax.axhline(y=0, color=\"gray\", linestyle=\"--\", alpha=0.5)\n",
    "ax.set_ylabel(\"Cart Position (m)\")\n",
    "ax.set_title(\"Cart Position Over Time\", fontweight=\"bold\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Actions\n",
    "ax = axes[2]\n",
    "ax.step(range(length), actions, where=\"post\", linewidth=2, color=\"crimson\")\n",
    "ax.set_ylabel(\"Action\\n(0=Left, 1=Right)\")\n",
    "ax.set_xlabel(\"Time Step\")\n",
    "ax.set_title(\"Actions Over Time\", fontweight=\"bold\")\n",
    "ax.set_ylim([-0.1, 1.1])\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Tutorial Complete!\n",
    "\n",
    "## What You Learned\n",
    "1. How to use `EvalConfig` for clean evaluation-only configuration\n",
    "2. How to work with `EvaluationResults` objects (pre-computed statistics + raw data)\n",
    "3. How to use `return_episodes=True` to collect full trajectories\n",
    "4. How to compare different agents using the platform\n",
    "\n",
    "## Key Platform Features\n",
    "- **`EvalConfig`**: Simplified config for evaluation (no training-specific fields)\n",
    "- **`EvaluationResults`**: Structured results with pre-computed statistics\n",
    "  - `results.mean_return`, `results.std_return`, etc.\n",
    "  - `results.episode_returns` (raw data for custom analysis)\n",
    "  - `results.episodes` (full trajectories if `return_episodes=True`)\n",
    "  - `results.summary()` (quick overview)\n",
    "- **Agent registry**: Classical controllers available via `AgentConfig(name=\"random\")`\n",
    "\n",
    "## API Comparison\n",
    "\n",
    "**Old approach (manual stats):**\n",
    "```python\n",
    "results = evaluate(config)  # Returns dict\n",
    "mean_return = np.mean(results['episode_return'])  # Manual computation\n",
    "```\n",
    "\n",
    "**New approach (structured results):**\n",
    "```python\n",
    "results = evaluate(config)  # Returns EvaluationResults\n",
    "mean_return = results.mean_return  # Pre-computed\n",
    "print(results.summary())  # Easy overview\n",
    "```\n",
    "\n",
    "## Next Steps\n",
    "Try:\n",
    "- Compare with learned agents from Tutorial 01 (use `Config` instead of `EvalConfig`)\n",
    "- Evaluate on different environment configurations\n",
    "- Use `results.episodes` to debug agent behavior\n",
    "- Test pre-trained models by passing `agent_state` to `evaluate()`\n",
    "- Enable W&B logging in `EvalConfig`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
